{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27b218d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer  # used for lemmatizer\n",
    "import pandas as pd\n",
    "import string\n",
    "import contractions\n",
    "from spellchecker import SpellChecker\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-dark-palette')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import demoji\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81384ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mansi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mansi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mansi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mansi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caac08d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================================================\n",
    "# Removing links (pre-processing)\n",
    "# ======================================================================================================================\n",
    "def strip_links(text):\n",
    "    all_links_regex = re.compile('http\\S+|www.\\S+', re.DOTALL)\n",
    "    text = re.sub(all_links_regex, '', text)\n",
    "    return text\n",
    "\n",
    "# ======================================================================================================================\n",
    "# Removing Punctuations (pre-processing)\n",
    "# ======================================================================================================================\n",
    "def remove_punctuation(text):\n",
    "    text = re.sub(r'@\\S+', '', text)  # Delete Usernames\n",
    "\n",
    "    # remove punctuation from each word (Replace hashtags with space, keeping hashtag context)\n",
    "    text = re.sub(r'#', '', text)  # Delete the hashtag sign\n",
    "\n",
    "    for separator in string.punctuation:\n",
    "        if separator not in [\"'\"]:\n",
    "            text = text.replace(separator, ' ')\n",
    "\n",
    "    return text\n",
    "\n",
    "# ======================================================================================================================\n",
    "# Removing Emojis (pre-processing)\n",
    "# ======================================================================================================================\n",
    "\n",
    "def remove_emoji(text):\n",
    "    text= demoji.replace(text, \"\") \n",
    "    return text\n",
    "\n",
    "# ======================================================================================================================\n",
    "# Removing Contractions (pre-processing)\n",
    "# ======================================================================================================================\n",
    "\n",
    "def get_contractions(text):\n",
    "    commonSMS = { \n",
    "    \"Ain’t\" : \"Am not\",\n",
    "    \"Wanna\" : \"Want to\",\n",
    "    \"Whatcha\" : \"What have you\",\n",
    "    \"Kinda\" : \"Kind of\",\n",
    "    \"Sorta\" : \"Sort of\",\n",
    "    \"Outta\" : \"Out of\",\n",
    "    \"Alotta\" : \"A lot of\",\n",
    "    \"Lotsa\" : \"Lots of\",\n",
    "    \"Mucha\" : \"Much of\",\n",
    "    \"Cuppa\" : \"Cup of\",\n",
    "    \"Dunno\" : \"Don’t know\",\n",
    "    \"Lemme\" : \"Let me\",\n",
    "    \"Gimme\" : \"Give me\",\n",
    "    \"Tell’em\" : \"Tell them\",\n",
    "    \"Imma\" : \"I am going to\",\n",
    "    \"Gonna\" : \"Going to\",\n",
    "    \"Needa\" : \"Need to\",\n",
    "    \"Oughta\" : \"Ought to\",\n",
    "    \"Hafta\" : \"Have to\",\n",
    "    \"Hasta\" : \"Has to\",\n",
    "    \"Usta\" : \"Used to\",\n",
    "    \"Supposta\" : \"Supposed to\",\n",
    "    \"Gotta\" : \"Got to\",\n",
    "    \"Cmon\" : \"Come on\",\n",
    "    \"Ya\" : \"You\",\n",
    "    \"Shoulda\" : \"Should have\",\n",
    "    \"Shouldna\" : \"Should not have\",\n",
    "    \"Wouldna\" : \"Would not have\",\n",
    "    \"She’da\" : \"She would have\",\n",
    "    \"Coulda\" : \"Could have\",\n",
    "    \"Woulda\" :\"Would have\",\n",
    "    \"Mighta\" : \"Might have\",\n",
    "    \"Mightna\" : \"Might not have\",\n",
    "    \"Musta\" : \"Must have\",\n",
    "    \"Mussna\" : \"Must not have\",\n",
    "    \"Dontcha\" : \"Do not you\",\n",
    "    \"Wontcha\" : \"Would not you\",\n",
    "    \"Whatcha\" : \"What are you\",\n",
    "    \"Betcha\" : \"Bet you\",\n",
    "    \"Gotcha\" : \"Got you\",\n",
    "    \"D’you\" : \"Do you\",\n",
    "    \"Didntcha\" : \"Did not you\",\n",
    "    \"Dija\" : \"Did you\",\n",
    "    \"S’more\" : \"Some more\",\n",
    "    \"Layder\" : \"Later\",\n",
    "    \"R\": \"are\",\n",
    "    \"N\":\"and\",\n",
    "    \"D\":'the',\n",
    "    \"BRB\":\"Be right back\",\n",
    "    \"IKR\":\"I know, right\",\n",
    "    \"ILY\":\"I love you\",\n",
    "    \"LMFAO\":\"Laughing my freaking ass off\",\n",
    "    \"NVM\": \"Never mind\",\n",
    "    \"OFC\": \"Of course\",\n",
    "    \"ROFL\":\"Rolling on the floor laughing\",\n",
    "    \"SMH\": \"Shaking my head\",\n",
    "    \"STFU\": \"Shut the fuck up\",\n",
    "    \"YOLO\": \"You only live once\",\n",
    "    \"MMB\":\"Message me back\",\n",
    "    \"YNT\":\"Why not\",\n",
    "    \"BW\":\"Between\",\n",
    "    \"TC\":\"Take care\",\n",
    "    \"MU\":\"Miss you\",\n",
    "    \"S2R\":\"Send to receive\",\n",
    "    \"NVM\":\"Never mind\",\n",
    "    \"CTN\":\"Can’t talk now\",\n",
    "    \"B4\":\"Before\",\n",
    "    \"FTW\":\"For the win\",\n",
    "    \"HW\":\"Homework\",\n",
    "    \"W8\":\"Wait\",\n",
    "    \"PC\":\"Personal computer\",\n",
    "    \"ITT\":\"In this thread\",\n",
    "    \"RBTL\":\"Read between the lines\",\n",
    "    \"ETA\":\"Estimated time of arrival\",\n",
    "    \"XOXO\":\"Hugs and kisses\",\n",
    "    \"AFK\":\"Away from keyboard\",\n",
    "    \"BuBye\":\"Bye Bye\",\n",
    "    \"DIY\":\"Do it yourself\",\n",
    "    \"MW\":\"On my way\",\n",
    "    \"SD\":\"Sweet dreams\",\n",
    "    \"YW\":\"You are welcome\",\n",
    "    \"RL\":\"Real life\",\n",
    "    \"SRY\":\"Sorry\",\n",
    "    \"DIKU\":\"Do I know you\",\n",
    "    \"IDGI\":\"I do not get it\",\n",
    "    \"IDC\":\"I do not care\",\n",
    "    \"IDK\":\"I do not know\",\n",
    "    \"CFY\":\"Calling for you\",\n",
    "    \"AAMOF\":\"As a matter of fact\",\n",
    "    \"TYT\":\"Take your time\",\n",
    "    \"TY\":\"Thank you\",\n",
    "    \"GG\":\"Good game\",\n",
    "    \"IRL\":\"In real life\",\n",
    "    \"GJ\":\"Good job\",\n",
    "    \"POV\":\"Point of view\",\n",
    "    \"R8\":\"Right\",\n",
    "    \"BTW\":\"By the way\",\n",
    "    \"SU\":\"Shut up\",\n",
    "    \"NC\":\"No comment\",\n",
    "    \"SEC\":\"Second\",\n",
    "    \"IMO\":\"In my opinion\",\n",
    "    \"JK\":\"Just kidding\",\n",
    "    \"KK\":\"Okay cool\",\n",
    "    \"PPL\":\"People\",\n",
    "    \"GTG\":\"Got to go\",\n",
    "    \"NP\":\"No problem\",\n",
    "    \"ROFL\":\"Rolling on the floor laughing\",\n",
    "    \"RIP\":\"Rest in peace\",\n",
    "    \"SMH\":\"Shaking my head\",\n",
    "    \"PLZ\":\"Please\",\n",
    "    \"RT\":\"Real time\",\n",
    "    \"CYL\":\"Call you later\",\n",
    "    \"GM\":\"Good morning\",\n",
    "    \"GR8\":\"Great\",\n",
    "    \"YOLO\":\"You only live once\",\n",
    "    \"GN\":\"Goodnight\",\n",
    "    \"WD\":\"Well done\",\n",
    "    \"TTYS\":\"Talk to you soon\",\n",
    "    \"BD\":\"Big deal\",\n",
    "    \"GL\":\"Good luck\",\n",
    "    \"L8R\":\"Later\",\n",
    "    \"TTYL\":\"Talk to you later\",\n",
    "    \"TMI\":\"Too much information\",\n",
    "    \"IM\":\"Instant message\",\n",
    "    \"ASIC\":\"As soon as I can\",\n",
    "    \"TCO\":\"Taken care of\",\n",
    "    \"BBIAB\":\"Be back in a bit\",\n",
    "    \"B4N\" :\"Bye for Now\",\n",
    "    \"HU\":\"Hug you\",\n",
    "    \"QT\":\"Cutie\",\n",
    "    \"MSG\":\"Message\",\n",
    "    \"LOL\":\"laugh out loud\",\n",
    "    \"ZZZ\":\"Sleeping\",\n",
    "    \"IC\":\"I see\",\n",
    "    \"JJ\":\"Just joking\",\n",
    "    \"F2F\":\"Face to face\",\n",
    "    \"BRB\":\"Be Right Back\",\n",
    "    \"CTN\":\"Can not talk now\",\n",
    "    \"TTYN\":\"Talk to you never\",\n",
    "    \"BFF\":\"Best Friends Forever\",\n",
    "    \"GBTW\":\"Get back to work\",\n",
    "    \"LMAO\":\"laughing my ass off\",\n",
    "    \"BC\":\"Because\",\n",
    "    \"PLS\":\"Please\",\n",
    "    \"NOOB\":\"Newbie\",\n",
    "    \"WTF\":\"What the fuck\",\n",
    "    \"CU\":\"See you\",\n",
    "    \"FAB\":\"Fabulous\",\n",
    "    \"THX\":\"Thanks\",\n",
    "    \"CUL\":\"See you later\",\n",
    "    \"COZ\":\"Because\",\n",
    "    \"CUZ\":\"Because\",\n",
    "    \"CAUSE\":\"Because\",\n",
    "    \"CYA\":\"See You\",\n",
    "    \"Y\":\"Why\",\n",
    "    \"TXT\":\"Text\",\n",
    "    \"KU\":\"Kiss you\",\n",
    "    \"FYI\":\"For your information\",\n",
    "    \"OOO\":\"Out of office\",\n",
    "    \"FAQ\":\"Frequently asked questions\",\n",
    "    \"LU\":\"Love you\",\n",
    "    \"AKA\":\"Also known as\",\n",
    "    \"THO\":\"Though\",\n",
    "    \"BAU\":\"Business as usual\",\n",
    "    \"HBU\":\"How about you\",\n",
    "    \"LMAO\":\"Laughing my ass off\",\n",
    "    \"AFAIK\" :\"As far as I know\",\n",
    "    \"BA3\":\"Battery\",\n",
    "    \"GMV\":\"Got my vote\",\n",
    "    \"RT\":\"Retweet\",\n",
    "    \"IMHO\":\"In my humble opinion\",\n",
    "    \"HTH\":\"Here to help\",\n",
    "    \"BF\":\"Boyfriend\",\n",
    "    \"PC\":\"Personal computer\",\n",
    "    \"L8\":\"Late\",\n",
    "    \"ASAP\":\"As soon as possible\",\n",
    "    \"GONNA\":\"Going to\",\n",
    "    \"GUNNA\":\"Going to\",\n",
    "    \"OMG\":\"Oh my God\",\n",
    "    \"LAM\":\"Leave a message\",\n",
    "    \"NTN\":\"No thanks needed\",\n",
    "    \"SS\":\"So sorry\",\n",
    "    \"M8\":\"Mate\",\n",
    "    \"2MORO\":\"Tomorrow\",\n",
    "    \"LNG\":\"Long\",\n",
    "    \"pic\":\"picture\",\n",
    "    \"OMG\":\"Oh my god\",\n",
    "    \"GAL\":\"Girl\",\n",
    "    \"DND\":\"Do not disturb\",\n",
    "    \"10Q\":\"Thank you\",\n",
    "    \"2B\":\"To be\",\n",
    "    \"4EVA\":\"Forever\",\n",
    "    \"2MOR\" :\"Tomorrow\",\n",
    "    \"YT\":\"YouTube\",\n",
    "    \"utube\": \"Youtube\",\n",
    "    \"der\":\"there\",\n",
    "    \"wrk\":\"work\",\n",
    "    \"tv\":\"television\",\n",
    "    \"lol\":\"Laugh out loud\",\n",
    "    \"4got\":\"Forgot\",\n",
    "    \"yr\":\"year\",\n",
    "    \"hr\":\"hour\",\n",
    "    \"b4\":\"before\",\n",
    "    \"bout\":\"about\",\n",
    "    \"c\":\"see\",\n",
    "    \"dat\":\"that\",\n",
    "    \"tellin\":\"telling\"    \n",
    "    }\n",
    "    new_dict = dict((k.lower(), v.lower()) for k, v in  commonSMS.items())   \n",
    "    text_decontracted = []\n",
    "\n",
    "    for word in text.split():\n",
    "        if word in new_dict:\n",
    "            word = new_dict[word]\n",
    "        text_decontracted.append(word)\n",
    "\n",
    "    text = ' '.join(text_decontracted)\n",
    "    return text\n",
    "\n",
    "# ======================================================================================================================\n",
    "# Removing Spelling mistakes (pre-processing)\n",
    "# ======================================================================================================================\n",
    "\n",
    "'''To perform spelling mistake correction, you first need to make sure the word \n",
    "is not absurd or from slang like, caaaar, amazzzing etc. with repeated alphabets. \n",
    "'''\n",
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "'''use Pyspellchecker library from Python for correcting spellings'''\n",
    "\n",
    "def spell_checker(text):\n",
    "    spell = SpellChecker()\n",
    "    text=' '.join(str(spell.correction(w)) for w in text.split())\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efd3c5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "        #lower text\n",
    "        text = text.lower().strip()\n",
    "\n",
    "        #remove punctuations and links\n",
    "        text = remove_punctuation(strip_links(text))\n",
    "\n",
    "        # remove emails\n",
    "        text = re.sub('\\S*@\\S*\\s?', '', text)\n",
    "\n",
    "        # remove rt and via in case of tweet data\n",
    "        text = re.sub(r\"\\b(rt|RT)\\b\", \"\", text)\n",
    "        text = re.sub(r\"\\b(via|VIA)\\b\", \"\", text)\n",
    "        text = re.sub(r\"\\b(it|IT)\\b\", \"\", text)\n",
    "        text = re.sub(r\"\\b(btu|BTu)\\b\", \"\", text)\n",
    "        text = re.sub(r\"\\b(bt|BT )\\b\", \"\", text)\n",
    "\n",
    "\n",
    "        # format contractions without apostrophe in order to use for contraction replacement\n",
    "        text = re.sub(r\"\\b(s|'s)\\b\", \" is \", text)\n",
    "        text = re.sub(r\"\\b(ve|'ve)\\b\", \" have \", text)\n",
    "        text = re.sub(r\"\\b(nt|'nt| 't)\\b\", \" not \", text)\n",
    "        text = re.sub(r\"\\b(re|'re)\\b\", \" are \", text)\n",
    "        text = re.sub(r\"\\b(d|'d)\\b\", \" would \", text)\n",
    "        text = re.sub(r\"\\b(ll|'ll)\\b\", \" will \", text)\n",
    "        text = re.sub(r\"\\b(m|'m)\\b\", \" am\", text)\n",
    "\n",
    "        '''replace consecutive non-ASCII characters with a space\n",
    "        examples=भारत (used for websites in India), 网络 (the .NET equivalent in China),קום(the .COM equivalent in Hebrew)\n",
    "                  இந்தியா (meaning ‘Tamil’ for India, which is a language spoken in parts of India)\n",
    "        '''\n",
    "        text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "\n",
    "        # remove emojis from text\n",
    "        #text = self.emoji_pattern.sub(r'', text)\n",
    "        text=remove_emoji(text)\n",
    "\n",
    "        # substitute contractions with full words\n",
    "        #english contractions\n",
    "        text=contractions.fix(text)\n",
    "        #Most common internet contractions\n",
    "        text = get_contractions(text)\n",
    "\n",
    "        #Check for spelling corrections\n",
    "        text=reduce_lengthening(text)\n",
    "        text=spell_checker(text)\n",
    "\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6aeb47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================================================\n",
    "# Creating list of Stop-words (pre-processing)\n",
    "# ======================================================================================================================\n",
    "new_stopwords=['say', 'get', 'go', 'know', 'may', 'need', 'make', 'see', 'want', 'come', 'take', 'use','life','money',\n",
    "               'little','even','head','right','eat','laugh','well','red','bad','best','year','today','watch','win','play',\n",
    "               'new','game','good','would', 'can', 'zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'may',\n",
    "           'also', 'across', 'among', 'beside', 'yet', 'within', 'mr', 'bbc', 'image', 'getty','woman','boy','guy'\n",
    "           'de', 'en', 'caption', 'copyright', 'something', 'tag', 'wait', 'set', 'put', 'add', 'post', 'give', 'way', 'check', 'think',\n",
    "          'www', 'must', 'look', 'call', 'minute', 'com', 'thing', 'much', 'happen','still','tell','talk','never','every,'\n",
    "          'quarantine', 'day', 'time', 'week', 'amp', 'find','None','man','girl','really','real','people','love','like','let','back' ]\n",
    "stop_words = set(list(stopwords.words('english')) + ['\"', '|'] + new_stopwords)\n",
    "\n",
    "# ======================================================================================================================\n",
    "# Creating list of Emoticons (pre-processing)\n",
    "# ======================================================================================================================\n",
    "# Happy Emoticons\n",
    "emoticons_happy = {':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}', ':^)', ':-D', ':D', '8-D',\n",
    "               '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D', '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P',\n",
    "               ':-P', ':P', 'X-P', 'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)', '<3'}\n",
    "\n",
    "# Sad Emoticons\n",
    "emoticons_sad = {':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<', ':-[', ':-<', '=\\\\', '=/',\n",
    "             '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c', ':c', ':{', '>:\\\\', ';('}\n",
    "\n",
    "# Combine sad and happy emoticons\n",
    "emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d963e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================================================\n",
    "# Removing Non-alphanumeric words (pre-processing)\n",
    "# ======================================================================================================================\n",
    "\n",
    "# function to keep only alpharethmetic values\n",
    "def only_alpha(tokenized_text):\n",
    "    text_alpha = []\n",
    "    for word in tokenized_text:\n",
    "        word_alpha = re.sub('[^a-z A-Z]+', ' ', word)\n",
    "        text_alpha.append(word_alpha)\n",
    "    return text_alpha\n",
    "\n",
    "# ======================================================================================================================\n",
    "# Applying Lemmatization(pre-processing)\n",
    "# ======================================================================================================================\n",
    "\n",
    "# convert POS tag to wordnet tag in order to use in lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def nltk_pos_tagger(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "#lemmatizing\n",
    "def lemmatizing(tokenized_text):\n",
    "\n",
    "    nltk_tagged = nltk.pos_tag(tokenized_text)  \n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_pos_tagger(x[1])), nltk_tagged)\n",
    "    lemma_list = []\n",
    "\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            lemma_list.append(word)\n",
    "        else:        \n",
    "            lemma_list.append(lemmatizer.lemmatize(word, tag))\n",
    "    return lemma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "969d89a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_lemma(text):\n",
    "        # tokenize text\n",
    "        tokenized_text = word_tokenize(text)\n",
    "\n",
    "        # remove all non alpharethmetic values\n",
    "        tokenized_text = only_alpha(tokenized_text)\n",
    "\n",
    "        # lemmatize / stem words\n",
    "        lemmatized_text =lemmatizing(tokenized_text)\n",
    "\n",
    "\n",
    "        filtered_text = []\n",
    "        # looping through conditions\n",
    "        for word in lemmatized_text:\n",
    "            word = word.strip()\n",
    "            # check tokens against stop words, emoticons and punctuations\n",
    "            # biggest english word: Pneumonoultramicroscopicsilicovolcanoconiosis (45 letters)\n",
    "            if (word not in stop_words and word not in emoticons and word not in string.punctuation\n",
    "                and not word.isspace() and len(word) > 1 and len(word) < 46):\n",
    "                \n",
    "                filtered_text.append(word)\n",
    "\n",
    "\n",
    "\n",
    "        return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c554b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import compress_fasttext\n",
    "small_model = compress_fasttext.models.CompressedFastTextKeyedVectors.load(\"cc.en.300.compressed.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21755215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += small_model[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:  # handling the case where the token is not in vocabulary\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d120e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=pickle.load(open('C:/Users/mansi/Desktop/Study/Hate Speech Detection/models/voting.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3bcb7407",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_clf=pickle.load(open('C:/Users/mansi/Desktop/Study/Hate Speech Detection/models/clf.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bdec8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    text=preprocessing(text)\n",
    "    text=preprocess_lemma(text)\n",
    "    wordvec_arrays = word_vector(text, 300)\n",
    "    y_pred=model.predict(wordvec_arrays)\n",
    "    \n",
    "    if y_pred==0:\n",
    "        return \"Hate Tweet\"\n",
    "    if y_pred==1:\n",
    "        return \"Offensive Tweet\"\n",
    "    if y_pred==2:\n",
    "        return \"Neither Offensive Nor Hate\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bb09e01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Offensive Tweet'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"Bitch die u sleazy whore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6fc08e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hate Tweet'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"Indians are stupid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c759b57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neither Offensive Nor Hate'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"I like chai more than coffee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a8391340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict1(text):\n",
    "    text=preprocessing(text)\n",
    "    text=preprocess_lemma(text)\n",
    "    wordvec_arrays = word_vector(text, 300)\n",
    "    y_pred=model_clf.predict(wordvec_arrays)\n",
    "    \n",
    "    if y_pred==0:\n",
    "        return \"Hate Tweet\"\n",
    "    if y_pred==1:\n",
    "        return \"Offensive Tweet\"\n",
    "    if y_pred==2:\n",
    "        return \"Neither Offensive Nor Hate\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8584250f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Offensive Tweet'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict1(\"Bitch die u sleazy whore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4360cbe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neither Offensive Nor Hate'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict1(\"I like chai more than coffee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a04d174f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Offensive Tweet'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"i’m glad you are alive but you so easily could have died and it would solely be your fault.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b2860e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hate Tweet'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"For someone writing a blog about how not to be a douche online, you do a pretty good job of being a total douche!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4e4c5fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hate Tweet'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"Black people are retards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5168e85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Offensive Tweet'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict1(\"You’re an idiot! I hope you learnt a valuable lesson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2768eb56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neither Offensive Nor Hate'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"You questioning the greatest country on earth takes me aback and makes me question why I follow you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "068979bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hate Tweet'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict1(\"I hate cow worshippers Hindu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d5786dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hate Tweet'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"Indians and beggars are not allowed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d573e86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Offensive Tweet'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict1(\"Indian men are sleazy and disgusting and so they are not welcomed in America\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa23d24f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
